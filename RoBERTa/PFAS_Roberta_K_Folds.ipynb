{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46-jMsToZ4nm"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio transformers scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx5bD6EoY4zy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, roc_auc_score, cohen_kappa_score\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w12eXN73O1Ws"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtPUmkpVY-sf"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Landcover > Discharger.csv\")  # Update path if needed\n",
        "\n",
        "# Map text labels to numeric\n",
        "label_mapping = {\"Low\": 0, \"Medium\": 1, \"High\" : 2}\n",
        "df[\"Label\"] = df[\"Label\"].map(label_mapping)\n",
        "texts = df[\"TextDescription\"].tolist()\n",
        "labels = df[\"Label\"].tolist()\n",
        "\n",
        "# Tokenize all at once\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "encodings = tokenizer(\n",
        "    texts,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SKx9RViZCtI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Custom Dataset class\n",
        "class PFASDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY_ZIHMxZIz4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create dataset object\n",
        "dataset = PFASDataset(encodings, labels)\n",
        "\n",
        "# K-Fold setup\n",
        "k_folds = 5\n",
        "skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
        "\n",
        "# Metrics trackers\n",
        "all_accuracies, all_precisions, all_recalls, all_f1s = [], [], [], []\n",
        "fold_names = []\n",
        "train_losses, test_losses = [], []\n",
        "train_kappas, test_kappas = [], []\n",
        "all_confidences, wrong_confidences = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81wXZuqoStqU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Begin K-Fold loop\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(texts, labels)):\n",
        "    print(f\"\\n===== Fold {fold + 1} =====\")\n",
        "    fold_names.append(f\"Fold {fold + 1}\")\n",
        "\n",
        "    # Create loaders\n",
        "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "    test_subset = torch.utils.data.Subset(dataset, test_idx)\n",
        "    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True)\n",
        "    test_loader = DataLoader(test_subset, batch_size=8, shuffle=False)\n",
        "\n",
        "    # Model setup\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
        "    num_epochs = 10\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    # Training loop\n",
        "    epoch_train_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        epoch_train_losses.append(avg_train_loss)\n",
        "        print(f\"Epoch {epoch+1} | Avg Loss: {avg_train_loss:.4f}\")\n",
        "    train_losses.append(epoch_train_losses[-1])  # Save final epoch's train loss\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_preds, all_true, all_probs = [], [], []\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_true.extend(batch['labels'].cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            test_loss += loss.item()\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    test_losses.append(avg_test_loss)\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(all_true, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_true, all_preds, average='weighted')\n",
        "    test_kappa = cohen_kappa_score(all_true, all_preds)\n",
        "\n",
        "    test_kappas.append(test_kappa)\n",
        "    conf_mat = confusion_matrix(all_true, all_preds)\n",
        "\n",
        "    print(f\"Fold {fold + 1} Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Kappa: {test_kappa:.4f}\")\n",
        "    print(classification_report(all_true, all_preds, digits=3))\n",
        "    print(\"Confusion Matrix:\\n\", conf_mat)\n",
        "\n",
        "    all_accuracies.append(acc)\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "    all_f1s.append(f1)\n",
        "\n",
        "     # Confidence analysis\n",
        "    confidences = np.max(all_probs, axis=1)\n",
        "    all_confidences.extend(confidences)\n",
        "    wrong_confidences.extend(confidences[np.array(all_preds) != np.array(all_true)])\n",
        "\n",
        "    # Plot confusion matrix for each fold\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"Fold {fold + 1} - Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "# Final average\n",
        "print(\"\\n==== Final Average Metrics Across Folds ====\")\n",
        "print(f\"Accuracy:  {np.mean(all_accuracies):.4f}\")\n",
        "print(f\"Precision: {np.mean(all_precisions):.4f}\")\n",
        "print(f\"Recall:    {np.mean(all_recalls):.4f}\")\n",
        "print(f\"F1 Score:  {np.mean(all_f1s):.4f}\")\n",
        "print(f\"Kappa Score: {np.mean(test_kappas):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53-wTlSqpIEU"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "all_confidences = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "        max_conf = torch.max(probs, dim=1).values\n",
        "        all_confidences.extend(max_conf.cpu().numpy())\n",
        "\n",
        "plt.hist(all_confidences, bins=20, edgecolor='black')\n",
        "plt.title(\"Histogram of Prediction Confidences\")\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58fA8B1uptOc"
      },
      "outputs": [],
      "source": [
        "#  Histogram: Confidence on incorrect predictions\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.hist(wrong_confidences, bins=20, edgecolor='black')\n",
        "plt.title(\"Confidence on Incorrect Predictions\")\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Reliability diagram (calibration curve)\n",
        "from sklearn.calibration import calibration_curve\n",
        "prob_true, prob_pred = calibration_curve(\n",
        "    y_true=(np.array(all_preds) == np.array(all_true)),\n",
        "    y_prob=all_confidences,\n",
        "    n_bins=10\n",
        ")\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(prob_pred, prob_true, marker='o', label='Model')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
        "plt.title(\"Reliability Diagram (Calibration Curve)\")\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvqFQt5dld9w"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plotting results\n",
        "df_results = pd.DataFrame({\n",
        "    \"Fold\": fold_names,\n",
        "    \"Accuracy\": all_accuracies,\n",
        "    \"Precision\": all_precisions,\n",
        "    \"Recall\": all_recalls,\n",
        "    \"F1 Score\": all_f1s\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for metric in [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]:\n",
        "    plt.plot(df_results[\"Fold\"], df_results[metric], marker='o', label=metric, linewidth=2)\n",
        "\n",
        "plt.title(\"Model Performance Across Folds\", fontsize=16)\n",
        "plt.xlabel(\"Fold\", fontsize=12)\n",
        "plt.ylabel(\"Score\", fontsize=12)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
